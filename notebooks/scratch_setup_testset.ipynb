{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d75629e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import fiftyone as fo\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from pathlib import Path\n",
    "from eyepop import EyePopSdk\n",
    "from eyepop.data.data_endpoint import DataEndpoint\n",
    "from eyepop.data.data_jobs import DataJob\n",
    "from eyepop.data.data_types import DatasetCreate, AssetImport, AutoAnnotateParams, Dataset, Asset, ChangeEvent, ChangeType, DatasetUpdate, UserReview, Model, ModelCreate, ModelStatus\n",
    "import asyncio\n",
    "from tqdm.notebook import tqdm\n",
    "import hashlib\n",
    "from fiftyone import Dataset\n",
    "from fiftyone.core.labels import Detection, Detections\n",
    "from fiftyone.core.metadata import ImageMetadata\n",
    "\n",
    "# üîß Configuration\n",
    "accountUUID = \"034cb8e37f5444e98a78f1be65fd0bff\"\n",
    "fullModelUUID = \"068819c3a65473ce800081b210833ff7\"\n",
    "fullModelQuantizedUUID = \"\" #technically same \n",
    "# https://localhost:3000/wizardModel?type=object&step=deploy&accountUUID=49326f2e085a46c39ba73f91c52e436c&modelUUID=067ad32ae1bf704b80003bc57fbe3694&datasetUUID=066f5aed921579b280009378da7c0049\n",
    "sub300ModelUUID = \"068818bdef1c7a3f800056a0dafbe761\"\n",
    "sub300ModelQuantizedUUID = \"\" # haven't quantized yet\n",
    "\n",
    "datasetUUID = \"068818885c547b1e80002649e95254ce\"\n",
    "fullDatasetQuantizedUUID = \"\"\n",
    "sub300datasetUUID = \"0688195453827ae280000526e30dc7a9\"\n",
    "sub300datasetQuantizedUUID = \"\" # haven't quantized yet\n",
    "\n",
    "modelLabel = {\n",
    "    datasetUUID: \"EyePop.ai Trained Model (full)\",\n",
    "    fullDatasetQuantizedUUID: \"EyePop.ai Trained Model (full, quantized)\",\n",
    "    sub300datasetUUID: \"EyePop.ai Trained Model (sub-300)\",\n",
    "    sub300datasetQuantizedUUID: \"EyePop.ai Trained Model (sub-300, quantized)\",\n",
    "}\n",
    "\n",
    "#apikey = \"AAE_w6lCcrCa27chNAbZO-WdZ0FBQUFBQmwyUFk5bmtLZnJBQ2RFVWVDbzU1MnkwTUMzYXhQWjA4a0ZEczFKWWdONjdRS0NGWUZ5aF90aXVQZ3FrcWdkZWwwUEx6Q0luM0F3b3ItMjdqRmhUQkxyTWVvSndFLWRCUENjZGNlanZhbGhRTDdtV289\"\n",
    "apikey = \"AAGcsWj8N2PlKQl9c9ydz3QFZ0FBQUFBQm1mZDB5eDUwalNlYi12NWotd3hsVGJiMW1sVXF1dE9aOU9oSGVBOWtBQXoxZmNjUE5Nb1YzY3RROUdzbVUwUkZtcDhZcG5vSWROTzR1TU8ybGhZckx6RTgzYVZwMjZEREZjalZubnpYaUNMWVdBODg9\"\n",
    "cache_directory = \"./.cache/voxel51/\" + accountUUID + \"/\" + sub300datasetUUID\n",
    "\n",
    "def checkCacheDirectory():\n",
    "    if not os.path.exists(os.path.expanduser(cache_directory)):\n",
    "        print(\"Cache directory does not exist. Creating it...\")\n",
    "        os.makedirs(os.path.expanduser(cache_directory))\n",
    "    cache_files = os.listdir(os.path.expanduser(cache_directory))\n",
    "    if not cache_files:\n",
    "        print(\"Cache directory is empty.\")\n",
    "        return False\n",
    "    print(\"Cache directory contains files:\", cache_files)    \n",
    "    return True\n",
    "\n",
    "def convert_annotations_to_coco(asset_uuid, annotations, image_id, starting_annotation_id=1):\n",
    "    if(not annotations or len(annotations) == 0):\n",
    "        print(\"No annotations found.\")\n",
    "        return None\n",
    "    \n",
    "    image_width = annotations[0].annotation.source_height\n",
    "    image_height = annotations[0].annotation.source_height\n",
    "    \n",
    "    coco = {\n",
    "        \"images\": [\n",
    "            {\n",
    "                \"id\": image_id,\n",
    "                \"width\": image_width,\n",
    "                \"height\": image_height,\n",
    "                \"file_name\": f\"{asset_uuid}.jpg\",\n",
    "            }\n",
    "        ],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": []\n",
    "    }\n",
    "\n",
    "    category_name_to_id = {}\n",
    "    annotation_id = starting_annotation_id\n",
    "\n",
    "    for annotation in annotations:\n",
    "        if not hasattr(annotation, \"annotation\") or not hasattr(annotation.annotation, \"objects\"):\n",
    "            continue\n",
    "\n",
    "        for obj in annotation.annotation.objects:\n",
    "            label = obj.classLabel\n",
    "            if label not in category_name_to_id:\n",
    "                category_id = len(category_name_to_id) + 1\n",
    "                category_name_to_id[label] = category_id\n",
    "                coco[\"categories\"].append({\n",
    "                    \"id\": category_id,\n",
    "                    \"name\": label,\n",
    "                    \"supercategory\": \"none\"\n",
    "                })\n",
    "            else:\n",
    "                category_id = category_name_to_id[label]\n",
    "\n",
    "            bbox = [\n",
    "                obj.x,\n",
    "                obj.y,\n",
    "                obj.width,\n",
    "                obj.height\n",
    "            ]\n",
    "            area = obj.width * obj.height\n",
    "\n",
    "            coco[\"annotations\"].append({\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": category_id,\n",
    "                \"bbox\": bbox,\n",
    "                \"area\": area,\n",
    "                \"iscrowd\": 0\n",
    "            })\n",
    "            annotation_id += 1\n",
    "\n",
    "    return coco\n",
    "\n",
    "async def downloadDatasetToCache(accountUUID, datasetUUID):\n",
    "    print(\"Downloading dataset from EyePop.ai to cache directory...\")\n",
    "    print(\"Account UUID:\", accountUUID)\n",
    "    print(\"Dataset UUID:\", datasetUUID)\n",
    "    cache_directory = \"./.cache/voxel51/\" + accountUUID + \"/\" + datasetUUID\n",
    "    print(\"Cache directory:\", cache_directory)\n",
    "    print(\"Using API key:\", apikey)\n",
    "\n",
    "\n",
    "\n",
    "    async with EyePopSdk.dataEndpoint(\n",
    "        #eyepop_url = \"https://dataset-api.staging.eyepop.xyz/\",\n",
    "        eyepop_url = \"https://web-api.staging.eyepop.xyz/\",\n",
    "        secret_key=apikey, \n",
    "        account_id=accountUUID, \n",
    "        is_async=True, \n",
    "        disable_ws=False) as endpoint:\n",
    "\n",
    "        asset_list = await endpoint.list_assets(dataset_uuid=datasetUUID, include_annotations=True)\n",
    "        print(f\"Found {len(asset_list)} assets in the dataset.\")\n",
    "        \n",
    "        os.makedirs(os.path.expanduser(cache_directory), exist_ok=True)\n",
    "        cache_path = Path(os.path.expanduser(cache_directory))\n",
    "        images_dir = cache_path / \"data\"\n",
    "        annotations_dir = cache_path / \"annotations\"\n",
    "        images_dir.mkdir(exist_ok=True)\n",
    "        annotations_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # write entire asset_list to a JSON file for debugging\n",
    "        asset_list_path = cache_path / \"eyepop_asset_list.json\"\n",
    "        asset_list_val_path = cache_path / \"eyepop_asset_list_val.json\"\n",
    "        def default_serializer(obj):\n",
    "            if hasattr(obj, \"isoformat\"):\n",
    "                return obj.isoformat()\n",
    "            raise TypeError(f\"Object of type {type(obj).__name__} is not JSON serializable\")\n",
    "        with open(asset_list_path, \"w\") as f:\n",
    "            json.dump([asset.model_dump() for asset in asset_list], f, indent=2, default=default_serializer)\n",
    "        print(f\"Asset list saved to {asset_list_path}\")\n",
    "\n",
    "        # make a copy of the asset_list and filter out assets that do not have \"partition\": \"val\"\n",
    "        asset_list_val = [asset for asset in asset_list if asset.partition == \"val\"]\n",
    "        with open(asset_list_val_path, \"w\") as f:\n",
    "            json.dump([asset.model_dump() for asset in asset_list_val], f, indent=2, default=default_serializer)\n",
    "        print(f\"Asset list VAL saved to {asset_list_val_path}\")\n",
    "\n",
    "        combined_coco = {\n",
    "            \"images\": [],\n",
    "            \"annotations\": [],\n",
    "            \"categories\": []\n",
    "        }\n",
    "        category_name_to_id = {}\n",
    "        uuid_to_image_id = {}\n",
    "        annotation_id = 1\n",
    "        next_image_id = 1\n",
    "\n",
    "        pbar = tqdm(asset_list, desc=\"Starting downloads\")\n",
    "        for asset in pbar:\n",
    "            pbar.set_description(f\"Downloading {asset.uuid}\")\n",
    "\n",
    "            image_path = images_dir / f\"{asset.uuid}.jpg\"\n",
    "            if not image_path.exists():\n",
    "                image_response = await endpoint.download_asset(asset.uuid, datasetUUID, dataset_version=None)\n",
    "                image_bytes = await image_response.read()\n",
    "                with open(image_path, \"wb\") as f:\n",
    "                    f.write(image_bytes)\n",
    "\n",
    "            # Assign numeric image_id\n",
    "            image_id = next_image_id\n",
    "            uuid_to_image_id[asset.uuid] = image_id\n",
    "            next_image_id += 1\n",
    "\n",
    "            metadata = convert_annotations_to_coco(\n",
    "                asset_uuid=asset.uuid,\n",
    "                annotations=asset.annotations,\n",
    "                image_id=image_id,\n",
    "                starting_annotation_id=annotation_id\n",
    "            )\n",
    "\n",
    "            if not metadata or \"images\" not in metadata or \"annotations\" not in metadata:\n",
    "                print(f\"‚ö†Ô∏è Skipping asset {asset.uuid} due to unexpected metadata format\")\n",
    "                continue\n",
    "\n",
    "            combined_coco[\"images\"].extend(metadata[\"images\"])\n",
    "\n",
    "            for ann in metadata[\"annotations\"]:\n",
    "                ann[\"id\"] = annotation_id\n",
    "                annotation_id += 1\n",
    "                combined_coco[\"annotations\"].append(ann)\n",
    "\n",
    "            for cat in metadata.get(\"categories\", []):\n",
    "                if cat[\"name\"] not in category_name_to_id:\n",
    "                    cat_id = len(category_name_to_id) + 1\n",
    "                    category_name_to_id[cat[\"name\"]] = cat_id\n",
    "                    combined_coco[\"categories\"].append({\n",
    "                        \"id\": cat_id,\n",
    "                        \"name\": cat[\"name\"],\n",
    "                        \"supercategory\": \"none\"\n",
    "                    })\n",
    "\n",
    "        # Normalize category IDs across annotations\n",
    "        name_to_id = {cat[\"name\"]: cat[\"id\"] for cat in combined_coco[\"categories\"]}\n",
    "        for ann in combined_coco[\"annotations\"]:\n",
    "            cat_id = ann[\"category_id\"]\n",
    "            cat_name = next((c[\"name\"] for c in combined_coco[\"categories\"] if c[\"id\"] == cat_id), None)\n",
    "            if cat_name:\n",
    "                ann[\"category_id\"] = name_to_id[cat_name]\n",
    "\n",
    "        annotations_path = annotations_dir / \"annotations.json\"\n",
    "        with open(annotations_path, \"w\") as f:\n",
    "            json.dump(combined_coco, f, indent=2)\n",
    "\n",
    "        await endpoint.disconnect()\n",
    "\n",
    "    print(\"‚úÖ Dataset downloaded successfully.\")\n",
    "    dataset = export_fiftyone_format(\n",
    "        asset_list=asset_list_val,\n",
    "        cache_dir=cache_directory,\n",
    "        datasetUUID=datasetUUID\n",
    "    )\n",
    "    print(\"‚úÖ FiftyOne dataset exported.\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def export_fiftyone_format(asset_list, cache_dir, datasetUUID):\n",
    "    name = f\"eyepop_dataset_{datasetUUID}\"\n",
    "    if name in fo.list_datasets():\n",
    "        print(f\"Dataset {name} already exists. Deleting it...\")\n",
    "        fo.delete_dataset(name)\n",
    "\n",
    "    dataset = Dataset(name=name)\n",
    "\n",
    "    for asset in asset_list:\n",
    "        image_path = os.path.join(cache_dir, \"data\", f\"{asset.uuid}.jpg\")\n",
    "        if not os.path.exists(image_path):\n",
    "            continue\n",
    "\n",
    "        sample = fo.Sample(\n",
    "            filepath=image_path,\n",
    "            tags=[asset.partition] if hasattr(asset, \"partition\") else [],\n",
    "            metadata=ImageMetadata(),  # ‚úÖ use correct type\n",
    "        )\n",
    "\n",
    "        # ‚úÖ Store custom metadata as separate fields\n",
    "        sample[\"partition\"] = getattr(asset, \"partition\", None)\n",
    "        sample[\"uuid\"] = asset.uuid\n",
    "        \n",
    "        detections = []\n",
    "        predictions_300 = []\n",
    "        predictions_3k = []\n",
    "        \n",
    "        if hasattr(asset, \"annotations\"):\n",
    "            for annotation in asset.annotations:\n",
    "                if not hasattr(annotation, \"annotation\") or not hasattr(annotation.annotation, \"objects\"):\n",
    "                    continue\n",
    "                for obj in annotation.annotation.objects:\n",
    "                    if(obj.classLabel != \"stenosis\"):\n",
    "                        continue\n",
    "\n",
    "                    if obj.confidence is None:\n",
    "                        if annotation.type != \"ground_truth\":\n",
    "                            continue\n",
    "                    elif obj.confidence < 0.6:\n",
    "                        continue\n",
    "\n",
    "                    source = annotation.source if annotation.source is not None else \"\"\n",
    "                    source_label= \"(EyePop 300 Model)\"\n",
    "                    if \"stenosis-ctktz\" in source:\n",
    "                        source_label= \"(EyePop 3k Model)\"\n",
    "\n",
    "                    if annotation.type == \"prediction\" or annotation.type == \"auto\":\n",
    "                        if(source_label == \"(EyePop 300 Model)\"):\n",
    "                            predictions_300.append(\n",
    "                                Detection(\n",
    "                                    label=obj.classLabel  +\" - \"+ (source_label),\n",
    "                                    bounding_box=[obj.x, obj.y, obj.width, obj.height],\n",
    "                                    confidence=obj.confidence if hasattr(obj, \"confidence\") else 0\n",
    "                                )\n",
    "                            )\n",
    "                        elif(source_label == \"(EyePop 3k Model)\"):\n",
    "                            predictions_3k.append(\n",
    "                                Detection(\n",
    "                                    label=obj.classLabel  +\" - \"+ (source_label),\n",
    "                                    bounding_box=[obj.x, obj.y, obj.width, obj.height],\n",
    "                                    confidence=obj.confidence if hasattr(obj, \"confidence\") else 0\n",
    "                                )\n",
    "                            )\n",
    "                    elif annotation.type == \"ground_truth\":\n",
    "                        #if(annotation.source is None):\n",
    "                        detections.append(\n",
    "                            Detection(\n",
    "                                label=obj.classLabel + \" - \" + (\"ground_truth\"),\n",
    "                                bounding_box=[obj.x, obj.y, obj.width, obj.height],\n",
    "                                confidence=1\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "        if detections:\n",
    "            sample[\"ground_truth\"] = Detections(detections=detections)\n",
    "            \n",
    "        if predictions_300:\n",
    "            sample[\"predictions_300\"] = Detections(detections=predictions_300)\n",
    "        \n",
    "        if predictions_3k:\n",
    "            sample[\"predictions_3k\"] = Detections(detections=predictions_3k)\n",
    "        \n",
    "\n",
    "        dataset.add_sample(sample)\n",
    "\n",
    "    export_path = os.path.join(cache_dir, \"fiftyone_dataset\")\n",
    "    dataset.export(\n",
    "        export_dir=export_path,\n",
    "        dataset_type=fo.types.FiftyOneDataset,\n",
    "        label_field=\"ground_truth\"\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ FiftyOne dataset exported to {export_path}\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "081fb683",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def getEyePopModel(accountUUID, modelUUID):\n",
    "    print(\"Fetching model from EyePop.ai...\")\n",
    "    print(\"Account UUID:\", accountUUID)\n",
    "    print(\"Model UUID:\", modelUUID)\n",
    "    print(\"Using API key:\", apikey)\n",
    "\n",
    "    async with EyePopSdk.dataEndpoint(\n",
    "        eyepop_url=\"https://web-api.staging.eyepop.xyz/\",\n",
    "        secret_key=apikey,\n",
    "        account_id=accountUUID,\n",
    "        is_async=True,\n",
    "        disable_ws=False,\n",
    "    ) as endpoint:\n",
    "        model = await endpoint.get_model(model_uuid=modelUUID)\n",
    "        await endpoint.disconnect()\n",
    "\n",
    "    if not model:\n",
    "        raise ValueError(f\"Model {modelUUID} not found for account {accountUUID}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# model1 = await getEyePopModel(accountUUID, fullModelUUID)\n",
    "# print(model1)\n",
    "\n",
    "# model2 = await getEyePopModel(accountUUID, sub300ModelUUID)\n",
    "# print(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f791cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache directory: ./.cache/voxel51/034cb8e37f5444e98a78f1be65fd0bff/0688195453827ae280000526e30dc7a9\n",
      "Downloading dataset from EyePop.ai to cache directory...\n",
      "Account UUID: 034cb8e37f5444e98a78f1be65fd0bff\n",
      "Dataset UUID: 068818885c547b1e80002649e95254ce\n",
      "Cache directory: ./.cache/voxel51/034cb8e37f5444e98a78f1be65fd0bff/068818885c547b1e80002649e95254ce\n",
      "Using API key: AAGcsWj8N2PlKQl9c9ydz3QFZ0FBQUFBQm1mZDB5eDUwalNlYi12NWotd3hsVGJiMW1sVXF1dE9aOU9oSGVBOWtBQXoxZmNjUE5Nb1YzY3RROUdzbVUwUkZtcDhZcG5vSWROTzR1TU8ybGhZckx6RTgzYVZwMjZEREZjalZubnpYaUNMWVdBODg9\n",
      "Found 299 assets in the dataset.\n",
      "Asset list saved to .cache/voxel51/034cb8e37f5444e98a78f1be65fd0bff/068818885c547b1e80002649e95254ce/eyepop_asset_list.json\n",
      "Asset list VAL saved to .cache/voxel51/034cb8e37f5444e98a78f1be65fd0bff/068818885c547b1e80002649e95254ce/eyepop_asset_list_val.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ebaf60325649f7895b6083820a5159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Starting downloads:   0%|          | 0/299 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset downloaded successfully.\n",
      "Dataset eyepop_dataset_068818885c547b1e80002649e95254ce already exists. Deleting it...\n",
      "Directory './.cache/voxel51/034cb8e37f5444e98a78f1be65fd0bff/068818885c547b1e80002649e95254ce/fiftyone_dataset' already exists; export will be merged with existing files\n",
      "Exporting samples...\n",
      " 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 121/121 [80.0ms elapsed, 0s remaining, 1.5K docs/s] \n",
      "‚úÖ FiftyOne dataset exported to ./.cache/voxel51/034cb8e37f5444e98a78f1be65fd0bff/068818885c547b1e80002649e95254ce/fiftyone_dataset\n",
      "‚úÖ FiftyOne dataset exported.\n",
      "Downloading dataset from EyePop.ai to cache directory...\n",
      "Account UUID: 034cb8e37f5444e98a78f1be65fd0bff\n",
      "Dataset UUID: 0688195453827ae280000526e30dc7a9\n",
      "Cache directory: ./.cache/voxel51/034cb8e37f5444e98a78f1be65fd0bff/0688195453827ae280000526e30dc7a9\n",
      "Using API key: AAGcsWj8N2PlKQl9c9ydz3QFZ0FBQUFBQm1mZDB5eDUwalNlYi12NWotd3hsVGJiMW1sVXF1dE9aOU9oSGVBOWtBQXoxZmNjUE5Nb1YzY3RROUdzbVUwUkZtcDhZcG5vSWROTzR1TU8ybGhZckx6RTgzYVZwMjZEREZjalZubnpYaUNMWVdBODg9\n",
      "Found 3000 assets in the dataset.\n",
      "Asset list saved to .cache/voxel51/034cb8e37f5444e98a78f1be65fd0bff/0688195453827ae280000526e30dc7a9/eyepop_asset_list.json\n",
      "Asset list VAL saved to .cache/voxel51/034cb8e37f5444e98a78f1be65fd0bff/0688195453827ae280000526e30dc7a9/eyepop_asset_list_val.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3f702f7b7945a3acaa7eb5f78438d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Starting downloads:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset downloaded successfully.\n",
      "Dataset eyepop_dataset_0688195453827ae280000526e30dc7a9 already exists. Deleting it...\n",
      "Directory './.cache/voxel51/034cb8e37f5444e98a78f1be65fd0bff/0688195453827ae280000526e30dc7a9/fiftyone_dataset' already exists; export will be merged with existing files\n",
      "Exporting samples...\n",
      " 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 121/121 [68.2ms elapsed, 0s remaining, 1.8K docs/s] \n",
      "‚úÖ FiftyOne dataset exported to ./.cache/voxel51/034cb8e37f5444e98a78f1be65fd0bff/0688195453827ae280000526e30dc7a9/fiftyone_dataset\n",
      "‚úÖ FiftyOne dataset exported.\n",
      "Loading dataset into FiftyOne...\n",
      "Connected to FiftyOne on port 5151 at localhost.\n",
      "If you are not connecting to a remote session, you may need to start a new session and specify a port\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5151/?notebook=True&subscription=10152c10-9055-4c94-97e5-6daa8eadf000\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x11d750710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "async def run():\n",
    "    print(\"Cache directory:\", cache_directory)\n",
    "    dataset = await downloadDatasetToCache(accountUUID, datasetUUID)\n",
    "    await downloadDatasetToCache(accountUUID, sub300datasetUUID)\n",
    "\n",
    "    for sample in dataset.take(10):\n",
    "        print(sample.filepath, sample.ground_truth.detections)\n",
    "       \n",
    "    print(\"Loading dataset into FiftyOne...\")    \n",
    "    session = fo.launch_app(dataset, browser=True)\n",
    "    \n",
    "await run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c48d449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser\n",
    "\n",
    "webbrowser.open(\"http://localhost:5151/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63efbe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def extract_val_assets():\n",
    "    dataset_path = Path(os.path.expanduser(cache_directory)) / \"eyepop_asset_list_val.json\"\n",
    "    if not dataset_path.exists():\n",
    "        print(f\"Dataset file {dataset_path} does not exist.\")\n",
    "        return []\n",
    "\n",
    "    with open(dataset_path, \"r\") as f:\n",
    "        data = json.load(f)  # Load as full array\n",
    "    \n",
    "    assets = [Asset.model_validate(obj) for obj in data]  # or pydantic.parse_obj_as(List[Asset], data)\n",
    "    return assets\n",
    "\n",
    "def extract_coco_val_annotations(val_assets):\n",
    "    annotations_path = Path(os.path.expanduser(cache_directory)) / \"annotations\" / \"annotations.json\"\n",
    "    if not annotations_path.exists():\n",
    "        print(f\"Annotations file {annotations_path} does not exist.\")\n",
    "        return None\n",
    "\n",
    "    with open(annotations_path, \"r\") as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    # Filter annotations for the validation assets\n",
    "    asset_uuids = {asset.uuid for asset in val_assets}\n",
    "    coco_data[\"images\"] = [img for img in coco_data[\"images\"] if img[\"file_name\"].replace(\".jpg\", \"\") in asset_uuids]\n",
    "\n",
    "    #save filtered annotations\n",
    "    filtered_annotations_path = Path(os.path.expanduser(cache_directory)) / \"annotations\" / \"annotations_val.json\"\n",
    "    with open(filtered_annotations_path, \"w\") as f:\n",
    "        json.dump(coco_data, f, indent=2)\n",
    "    print(f\"Filtered annotations saved to {filtered_annotations_path}\")\n",
    "    \n",
    "    return coco_data\n",
    "\n",
    "async def run():\n",
    "    assets = extract_val_assets()\n",
    "    print(f\"Extracted {len(assets)} assets from the dataset.\")\n",
    "    #list all asset UUIDs\n",
    "    # for asset in assets:\n",
    "    #     print(f\"Asset UUID: {asset.uuid}, Partition: {asset.partition}, Annotations: {len(asset.annotations)}\")\n",
    "    \n",
    "    coco_annotations = extract_coco_val_annotations(assets)\n",
    "\n",
    "    print(\"Loading dataset into FiftyOne...\")    \n",
    "    dataset = fo.Dataset.from_dir(\n",
    "        dataset_dir=os.path.expanduser(cache_directory),\n",
    "        dataset_type=fo.types.COCODetectionDataset,\n",
    "        labels_path=\"annotations/annotations_val.json\",\n",
    "    )\n",
    "    session = fo.launch_app(dataset, browser=True)\n",
    "\n",
    "    # print the first 10 samples in the val set\n",
    "    for sample in dataset.take(10):\n",
    "        print(sample.filepath, sample.ground_truth.detections)\n",
    "# await run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f587b4f",
   "metadata": {},
   "source": [
    "Actions:\n",
    "- Ensure github is setup with notebooks and data (dataset full, dataset 300, combine into fiftyone view)\n",
    "- All thumbdrives loaded with github image\n",
    "- Script to copy in model into person's account\n",
    "- Test images for preview\n",
    "- load 3k model onto snapdragon device with 10 images (same test images)\n",
    "- \n",
    "\n",
    "# Step Setup\n",
    "- Setup notebook \n",
    "- Setup EyePop account\n",
    "- Copy API Key\n",
    "- Copy in 3k model into everyone's account\n",
    "\n",
    "# Notebook 01 \n",
    "- Setup dataset and notebooks locally with thumbdrive or github\n",
    "- load groundtruth of data into fiftyone\n",
    "- Explore main values of FiftyOne: dataset exploration, visualization of predictions and model evaluation\n",
    "\n",
    "# Notebook 02\n",
    "Load dataset (300) into EyePop.ai training system and open to correct spot\n",
    "\n",
    "# Step Break\n",
    "Social & Eating time\n",
    "\n",
    "# EP Product\n",
    "- Explore EyePop.ai training system\n",
    "- Explore Deploy and preview on EyePop.ai for model copied in\n",
    "- AI Hub demo for deploying locally to Snapdragon devices\n",
    "\n",
    "# Notebook 03 \n",
    "- Re- Open fiftyone and Explore differences between 300 model and 3k model (and ground truth)\n",
    "- More data for production 200k...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1bb714",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

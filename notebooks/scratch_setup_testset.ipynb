{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75629e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import fiftyone as fo\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from pathlib import Path\n",
    "from eyepop import EyePopSdk\n",
    "from eyepop.data.data_endpoint import DataEndpoint\n",
    "from eyepop.data.data_jobs import DataJob\n",
    "from eyepop.data.data_types import DatasetCreate, AssetImport, AutoAnnotateParams, Dataset, Asset, ChangeEvent, ChangeType, DatasetUpdate, UserReview, Model, ModelCreate, ModelStatus\n",
    "import asyncio\n",
    "from tqdm.notebook import tqdm\n",
    "import hashlib\n",
    "from fiftyone import Dataset\n",
    "from fiftyone.core.labels import Detection, Detections\n",
    "from fiftyone.core.metadata import ImageMetadata\n",
    "\n",
    "# üîß Configuration\n",
    "accountUUID = \"034cb8e37f5444e98a78f1be65fd0bff\"\n",
    "fullModelUUID = \"0687884e19e87bb38000eda578e1e386\"\n",
    "fullModelQuantizedUUID = \"\" #technically same \n",
    "# https://localhost:3000/wizardModel?type=object&step=deploy&accountUUID=49326f2e085a46c39ba73f91c52e436c&modelUUID=067ad32ae1bf704b80003bc57fbe3694&datasetUUID=066f5aed921579b280009378da7c0049\n",
    "sub300ModelUUID = \"\" #blythe running this\n",
    "sub300ModelQuantizedUUID = \"\" # haven't quantized yet\n",
    "\n",
    "datasetUUID = \"068775771e0a7d2e8000aa10e818018a\"\n",
    "fullDatasetQuantizedUUID = \"\"\n",
    "sub300datasetUUID = \"06881580891175618000bf9989ccd2e5\"\n",
    "sub300datasetQuantizedUUID = \"\" # haven't quantized yet\n",
    "\n",
    "modelLabel = {\n",
    "    datasetUUID: \"EyePop.ai Trained Model (full)\",\n",
    "    fullDatasetQuantizedUUID: \"EyePop.ai Trained Model (full, quantized)\",\n",
    "    sub300datasetUUID: \"EyePop.ai Trained Model (sub-300)\",\n",
    "    sub300datasetQuantizedUUID: \"EyePop.ai Trained Model (sub-300, quantized)\",\n",
    "}\n",
    "\n",
    "#apikey = \"AAE_w6lCcrCa27chNAbZO-WdZ0FBQUFBQmwyUFk5bmtLZnJBQ2RFVWVDbzU1MnkwTUMzYXhQWjA4a0ZEczFKWWdONjdRS0NGWUZ5aF90aXVQZ3FrcWdkZWwwUEx6Q0luM0F3b3ItMjdqRmhUQkxyTWVvSndFLWRCUENjZGNlanZhbGhRTDdtV289\"\n",
    "apikey = \"AAGcsWj8N2PlKQl9c9ydz3QFZ0FBQUFBQm1mZDB5eDUwalNlYi12NWotd3hsVGJiMW1sVXF1dE9aOU9oSGVBOWtBQXoxZmNjUE5Nb1YzY3RROUdzbVUwUkZtcDhZcG5vSWROTzR1TU8ybGhZckx6RTgzYVZwMjZEREZjalZubnpYaUNMWVdBODg9\"\n",
    "cache_directory = \"./.cache/voxel51/\" + accountUUID + \"/\" + datasetUUID\n",
    "\n",
    "def checkCacheDirectory():\n",
    "    if not os.path.exists(os.path.expanduser(cache_directory)):\n",
    "        print(\"Cache directory does not exist. Creating it...\")\n",
    "        os.makedirs(os.path.expanduser(cache_directory))\n",
    "    cache_files = os.listdir(os.path.expanduser(cache_directory))\n",
    "    if not cache_files:\n",
    "        print(\"Cache directory is empty.\")\n",
    "        return False\n",
    "    print(\"Cache directory contains files:\", cache_files)    \n",
    "    return True\n",
    "\n",
    "def convert_annotations_to_coco(asset_uuid, annotations, image_id, starting_annotation_id=1):\n",
    "    if(not annotations or len(annotations) == 0):\n",
    "        print(\"No annotations found.\")\n",
    "        return None\n",
    "    \n",
    "    image_width = annotations[0].annotation.source_height\n",
    "    image_height = annotations[0].annotation.source_height\n",
    "    \n",
    "    coco = {\n",
    "        \"images\": [\n",
    "            {\n",
    "                \"id\": image_id,\n",
    "                \"width\": image_width,\n",
    "                \"height\": image_height,\n",
    "                \"file_name\": f\"{asset_uuid}.jpg\",\n",
    "            }\n",
    "        ],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": []\n",
    "    }\n",
    "\n",
    "    category_name_to_id = {}\n",
    "    annotation_id = starting_annotation_id\n",
    "\n",
    "    for annotation in annotations:\n",
    "        if not hasattr(annotation, \"annotation\") or not hasattr(annotation.annotation, \"objects\"):\n",
    "            continue\n",
    "\n",
    "        for obj in annotation.annotation.objects:\n",
    "            label = obj.classLabel\n",
    "            if label not in category_name_to_id:\n",
    "                category_id = len(category_name_to_id) + 1\n",
    "                category_name_to_id[label] = category_id\n",
    "                coco[\"categories\"].append({\n",
    "                    \"id\": category_id,\n",
    "                    \"name\": label,\n",
    "                    \"supercategory\": \"none\"\n",
    "                })\n",
    "            else:\n",
    "                category_id = category_name_to_id[label]\n",
    "\n",
    "            bbox = [\n",
    "                obj.x,\n",
    "                obj.y,\n",
    "                obj.width,\n",
    "                obj.height\n",
    "            ]\n",
    "            area = obj.width * obj.height\n",
    "\n",
    "            coco[\"annotations\"].append({\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": category_id,\n",
    "                \"bbox\": bbox,\n",
    "                \"area\": area,\n",
    "                \"iscrowd\": 0\n",
    "            })\n",
    "            annotation_id += 1\n",
    "\n",
    "    return coco\n",
    "\n",
    "async def downloadDatasetToCache(accountUUID, datasetUUID):\n",
    "    print(\"Downloading dataset from EyePop.ai to cache directory...\")\n",
    "    print(\"Account UUID:\", accountUUID)\n",
    "    print(\"Dataset UUID:\", datasetUUID)\n",
    "    print(\"Cache directory:\", cache_directory)\n",
    "    print(\"Using API key:\", apikey)\n",
    "\n",
    "    async with EyePopSdk.dataEndpoint(\n",
    "        #eyepop_url = \"https://dataset-api.staging.eyepop.xyz/\",\n",
    "        eyepop_url = \"https://web-api.staging.eyepop.xyz/\",\n",
    "        secret_key=apikey, \n",
    "        account_id=accountUUID, \n",
    "        is_async=True, \n",
    "        disable_ws=False) as endpoint:\n",
    "\n",
    "        asset_list = await endpoint.list_assets(dataset_uuid=datasetUUID, include_annotations=True)\n",
    "        print(f\"Found {len(asset_list)} assets in the dataset.\")\n",
    "        \n",
    "        os.makedirs(os.path.expanduser(cache_directory), exist_ok=True)\n",
    "        cache_path = Path(os.path.expanduser(cache_directory))\n",
    "        images_dir = cache_path / \"data\"\n",
    "        annotations_dir = cache_path / \"annotations\"\n",
    "        images_dir.mkdir(exist_ok=True)\n",
    "        annotations_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # write entire asset_list to a JSON file for debugging\n",
    "        asset_list_path = cache_path / \"eyepop_asset_list.json\"\n",
    "        asset_list_val_path = cache_path / \"eyepop_asset_list_val.json\"\n",
    "        def default_serializer(obj):\n",
    "            if hasattr(obj, \"isoformat\"):\n",
    "                return obj.isoformat()\n",
    "            raise TypeError(f\"Object of type {type(obj).__name__} is not JSON serializable\")\n",
    "        with open(asset_list_path, \"w\") as f:\n",
    "            json.dump([asset.model_dump() for asset in asset_list], f, indent=2, default=default_serializer)\n",
    "        print(f\"Asset list saved to {asset_list_path}\")\n",
    "\n",
    "        # make a copy of the asset_list and filter out assets that do not have \"partition\": \"val\"\n",
    "        asset_list_val = [asset for asset in asset_list if asset.partition == \"val\"]\n",
    "        with open(asset_list_val_path, \"w\") as f:\n",
    "            json.dump([asset.model_dump() for asset in asset_list_val], f, indent=2, default=default_serializer)\n",
    "        print(f\"Asset list VAL saved to {asset_list_val_path}\")\n",
    "\n",
    "        combined_coco = {\n",
    "            \"images\": [],\n",
    "            \"annotations\": [],\n",
    "            \"categories\": []\n",
    "        }\n",
    "        category_name_to_id = {}\n",
    "        uuid_to_image_id = {}\n",
    "        annotation_id = 1\n",
    "        next_image_id = 1\n",
    "\n",
    "        pbar = tqdm(asset_list, desc=\"Starting downloads\")\n",
    "        for asset in pbar:\n",
    "            pbar.set_description(f\"Downloading {asset.uuid}\")\n",
    "\n",
    "            image_path = images_dir / f\"{asset.uuid}.jpg\"\n",
    "            if not image_path.exists():\n",
    "                image_response = await endpoint.download_asset(asset.uuid, datasetUUID, dataset_version=None)\n",
    "                image_bytes = await image_response.read()\n",
    "                with open(image_path, \"wb\") as f:\n",
    "                    f.write(image_bytes)\n",
    "\n",
    "            # Assign numeric image_id\n",
    "            image_id = next_image_id\n",
    "            uuid_to_image_id[asset.uuid] = image_id\n",
    "            next_image_id += 1\n",
    "\n",
    "            metadata = convert_annotations_to_coco(\n",
    "                asset_uuid=asset.uuid,\n",
    "                annotations=asset.annotations,\n",
    "                image_id=image_id,\n",
    "                starting_annotation_id=annotation_id\n",
    "            )\n",
    "\n",
    "            if not metadata or \"images\" not in metadata or \"annotations\" not in metadata:\n",
    "                print(f\"‚ö†Ô∏è Skipping asset {asset.uuid} due to unexpected metadata format\")\n",
    "                continue\n",
    "\n",
    "            combined_coco[\"images\"].extend(metadata[\"images\"])\n",
    "\n",
    "            for ann in metadata[\"annotations\"]:\n",
    "                ann[\"id\"] = annotation_id\n",
    "                annotation_id += 1\n",
    "                combined_coco[\"annotations\"].append(ann)\n",
    "\n",
    "            for cat in metadata.get(\"categories\", []):\n",
    "                if cat[\"name\"] not in category_name_to_id:\n",
    "                    cat_id = len(category_name_to_id) + 1\n",
    "                    category_name_to_id[cat[\"name\"]] = cat_id\n",
    "                    combined_coco[\"categories\"].append({\n",
    "                        \"id\": cat_id,\n",
    "                        \"name\": cat[\"name\"],\n",
    "                        \"supercategory\": \"none\"\n",
    "                    })\n",
    "\n",
    "        # Normalize category IDs across annotations\n",
    "        name_to_id = {cat[\"name\"]: cat[\"id\"] for cat in combined_coco[\"categories\"]}\n",
    "        for ann in combined_coco[\"annotations\"]:\n",
    "            cat_id = ann[\"category_id\"]\n",
    "            cat_name = next((c[\"name\"] for c in combined_coco[\"categories\"] if c[\"id\"] == cat_id), None)\n",
    "            if cat_name:\n",
    "                ann[\"category_id\"] = name_to_id[cat_name]\n",
    "\n",
    "        annotations_path = annotations_dir / \"annotations.json\"\n",
    "        with open(annotations_path, \"w\") as f:\n",
    "            json.dump(combined_coco, f, indent=2)\n",
    "\n",
    "        await endpoint.disconnect()\n",
    "\n",
    "    print(\"‚úÖ Dataset downloaded successfully.\")\n",
    "    dataset = export_fiftyone_format(\n",
    "        asset_list=asset_list_val,\n",
    "        cache_dir=cache_directory,\n",
    "        datasetUUID=datasetUUID\n",
    "    )\n",
    "    print(\"‚úÖ FiftyOne dataset exported.\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def export_fiftyone_format(asset_list, cache_dir, datasetUUID):\n",
    "    name = f\"eyepop_dataset_{datasetUUID}\"\n",
    "    if name in fo.list_datasets():\n",
    "        print(f\"Dataset {name} already exists. Deleting it...\")\n",
    "        fo.delete_dataset(name)\n",
    "\n",
    "    dataset = Dataset(name=name)\n",
    "\n",
    "    for asset in asset_list:\n",
    "        image_path = os.path.join(cache_dir, \"data\", f\"{asset.uuid}.jpg\")\n",
    "        if not os.path.exists(image_path):\n",
    "            continue\n",
    "\n",
    "        sample = fo.Sample(\n",
    "            filepath=image_path,\n",
    "            tags=[asset.partition] if hasattr(asset, \"partition\") else [],\n",
    "            metadata=ImageMetadata(),  # ‚úÖ use correct type\n",
    "        )\n",
    "\n",
    "        # ‚úÖ Store custom metadata as separate fields\n",
    "        sample[\"partition\"] = getattr(asset, \"partition\", None)\n",
    "        sample[\"uuid\"] = asset.uuid\n",
    "        \n",
    "        detections = []\n",
    "        predictions = []\n",
    "        if hasattr(asset, \"annotations\"):\n",
    "            for annotation in asset.annotations:\n",
    "                if not hasattr(annotation, \"annotation\") or not hasattr(annotation.annotation, \"objects\"):\n",
    "                    continue\n",
    "                for obj in annotation.annotation.objects:\n",
    "                    if(obj.classLabel != \"stenosis\"):\n",
    "                        continue\n",
    "\n",
    "                    if(obj.confidence < 0.5):\n",
    "                        continue\n",
    "\n",
    "                    if annotation.type == \"prediction\" or annotation.type == \"auto\":\n",
    "                        predictions.append(\n",
    "                            Detection(\n",
    "                                label=obj.classLabel  +\" - \"+ modelLabel.get(datasetUUID, \"Unknown Model\"),\n",
    "                                bounding_box=[obj.x, obj.y, obj.width, obj.height],\n",
    "                                confidence=obj.confidence if hasattr(obj, \"confidence\") else 0\n",
    "                            )\n",
    "                        )\n",
    "                    elif annotation.type == \"ground_truth\":\n",
    "                        #if(annotation.source is None):\n",
    "                        detections.append(\n",
    "                            Detection(\n",
    "                                label=obj.classLabel + (annotation.source if annotation.source is not None else \"\"),\n",
    "                                bounding_box=[obj.x, obj.y, obj.width, obj.height],\n",
    "                                confidence=1\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "        if detections:\n",
    "            sample[\"ground_truth\"] = Detections(detections=detections)\n",
    "            \n",
    "        if predictions:\n",
    "            sample[\"predictions\"] = Detections(detections=predictions)\n",
    "\n",
    "        dataset.add_sample(sample)\n",
    "\n",
    "    export_path = os.path.join(cache_dir, \"fiftyone_dataset\")\n",
    "    dataset.export(\n",
    "        export_dir=export_path,\n",
    "        dataset_type=fo.types.FiftyOneDataset,\n",
    "        label_field=\"ground_truth\"\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ FiftyOne dataset exported to {export_path}\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f791cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache directory: ./.cache/voxel51/034cb8e37f5444e98a78f1be65fd0bff/068775771e0a7d2e8000aa10e818018a\n",
      "Downloading dataset from EyePop.ai to cache directory...\n",
      "Account UUID: 034cb8e37f5444e98a78f1be65fd0bff\n",
      "Dataset UUID: 068775771e0a7d2e8000aa10e818018a\n",
      "Cache directory: ./.cache/voxel51/034cb8e37f5444e98a78f1be65fd0bff/068775771e0a7d2e8000aa10e818018a\n",
      "Using API key: AAGcsWj8N2PlKQl9c9ydz3QFZ0FBQUFBQm1mZDB5eDUwalNlYi12NWotd3hsVGJiMW1sVXF1dE9aOU9oSGVBOWtBQXoxZmNjUE5Nb1YzY3RROUdzbVUwUkZtcDhZcG5vSWROTzR1TU8ybGhZckx6RTgzYVZwMjZEREZjalZubnpYaUNMWVdBODg9\n",
      "Found 3000 assets in the dataset.\n",
      "Asset list saved to .cache/voxel51/034cb8e37f5444e98a78f1be65fd0bff/068775771e0a7d2e8000aa10e818018a/eyepop_asset_list.json\n",
      "Asset list VAL saved to .cache/voxel51/034cb8e37f5444e98a78f1be65fd0bff/068775771e0a7d2e8000aa10e818018a/eyepop_asset_list_val.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cbff00662ba4bc78bf4eb48cf56d8a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Starting downloads:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset downloaded successfully.\n",
      "Dataset eyepop_dataset_068775771e0a7d2e8000aa10e818018a already exists. Deleting it...\n",
      "Directory './.cache/voxel51/034cb8e37f5444e98a78f1be65fd0bff/068775771e0a7d2e8000aa10e818018a/fiftyone_dataset' already exists; export will be merged with existing files\n",
      "Exporting samples...\n",
      " 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 179/179 [173.7ms elapsed, 0s remaining, 1.0K docs/s]       \n",
      "‚úÖ FiftyOne dataset exported to ./.cache/voxel51/034cb8e37f5444e98a78f1be65fd0bff/068775771e0a7d2e8000aa10e818018a/fiftyone_dataset\n",
      "‚úÖ FiftyOne dataset exported.\n",
      "Images directory: ['0687757e173e722e800053a8af29fa7f.jpg', '068775798299716580005f08e7ff5011.jpg', '0687757893eb72348000888fd91003ca.jpg', '0687757b64df713f8000ae1eb6450822.jpg', '0687757bb58c7b488000a7233d92f98c.jpg']\n",
      "Annotations file exists: True\n",
      "Total images in COCO: 3000\n",
      "Total annotations: 65628\n",
      "Loading dataset into FiftyOne...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5151/?notebook=True&subscription=0b923424-cedd-4e65-9a3d-d513a70b08a0\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1325c5650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "async def run():\n",
    "    print(\"Cache directory:\", cache_directory)\n",
    "    dataset = await downloadDatasetToCache(accountUUID, datasetUUID)\n",
    "\n",
    "    print(\"Images directory:\", os.listdir(os.path.join(cache_directory, \"data\"))[:5])\n",
    "    print(\"Annotations file exists:\", os.path.exists(os.path.join(cache_directory, \"annotations\", \"annotations.json\")))\n",
    "    with open(os.path.join(cache_directory, \"annotations\", \"annotations.json\")) as f:\n",
    "        coco = json.load(f)\n",
    "        print(\"Total images in COCO:\", len(coco[\"images\"]))\n",
    "        print(\"Total annotations:\", len(coco[\"annotations\"]))\n",
    "\n",
    "   \n",
    "    print(\"Loading dataset into FiftyOne...\")    \n",
    "    \n",
    "    session = fo.launch_app(dataset, browser=True)\n",
    "    \n",
    "await run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c48d449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser\n",
    "\n",
    "webbrowser.open(\"http://localhost:5151/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63efbe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def extract_val_assets():\n",
    "    dataset_path = Path(os.path.expanduser(cache_directory)) / \"eyepop_asset_list_val.json\"\n",
    "    if not dataset_path.exists():\n",
    "        print(f\"Dataset file {dataset_path} does not exist.\")\n",
    "        return []\n",
    "\n",
    "    with open(dataset_path, \"r\") as f:\n",
    "        data = json.load(f)  # Load as full array\n",
    "    \n",
    "    assets = [Asset.model_validate(obj) for obj in data]  # or pydantic.parse_obj_as(List[Asset], data)\n",
    "    return assets\n",
    "\n",
    "def extract_coco_val_annotations(val_assets):\n",
    "    annotations_path = Path(os.path.expanduser(cache_directory)) / \"annotations\" / \"annotations.json\"\n",
    "    if not annotations_path.exists():\n",
    "        print(f\"Annotations file {annotations_path} does not exist.\")\n",
    "        return None\n",
    "\n",
    "    with open(annotations_path, \"r\") as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    # Filter annotations for the validation assets\n",
    "    asset_uuids = {asset.uuid for asset in val_assets}\n",
    "    coco_data[\"images\"] = [img for img in coco_data[\"images\"] if img[\"file_name\"].replace(\".jpg\", \"\") in asset_uuids]\n",
    "\n",
    "    #save filtered annotations\n",
    "    filtered_annotations_path = Path(os.path.expanduser(cache_directory)) / \"annotations\" / \"annotations_val.json\"\n",
    "    with open(filtered_annotations_path, \"w\") as f:\n",
    "        json.dump(coco_data, f, indent=2)\n",
    "    print(f\"Filtered annotations saved to {filtered_annotations_path}\")\n",
    "    \n",
    "    return coco_data\n",
    "\n",
    "async def run():\n",
    "    assets = extract_val_assets()\n",
    "    print(f\"Extracted {len(assets)} assets from the dataset.\")\n",
    "    #list all asset UUIDs\n",
    "    # for asset in assets:\n",
    "    #     print(f\"Asset UUID: {asset.uuid}, Partition: {asset.partition}, Annotations: {len(asset.annotations)}\")\n",
    "    \n",
    "    coco_annotations = extract_coco_val_annotations(assets)\n",
    "\n",
    "    print(\"Loading dataset into FiftyOne...\")    \n",
    "    dataset = fo.Dataset.from_dir(\n",
    "        dataset_dir=os.path.expanduser(cache_directory),\n",
    "        dataset_type=fo.types.COCODetectionDataset,\n",
    "        labels_path=\"annotations/annotations_val.json\",\n",
    "    )\n",
    "    session = fo.launch_app(dataset, browser=True)\n",
    "    \n",
    "# await run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
